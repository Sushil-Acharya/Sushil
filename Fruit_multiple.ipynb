{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fruit_multiple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WfTl9Q7bre7jcbQbEduUlSb3Bw68UcjG",
      "authorship_tag": "ABX9TyNz8DjxzbpKHGBt0sX+jVqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sushil-Acharya/Sushil/blob/main/Fruit_multiple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvHMxrBk6UwQ"
      },
      "source": [
        "# Import some required libraries. Some libraries will be imported later if needed.\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import keras \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense,Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adamax\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y6wo6du7k6x",
        "outputId": "a7c660ea-66bc-4f4d-9fa9-1913c1801159"
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "training_directory = \"/content/drive/My Drive/Colab Notebooks/Ours/\"\n",
        "\n",
        "def load_images(image_path):\n",
        "    data = load_files(image_path)\n",
        "    files = np.array(data['filenames'])\n",
        "    targets = np.array(data['target'])\n",
        "    target_labels = np.array(data['target_names'])\n",
        "    return files,targets,target_labels\n",
        "    \n",
        "x_train, y_train,target_labels = load_images(training_directory)\n",
        "\n",
        "\n",
        "print('Successfully loaded !')\n",
        "print('Number of training images : ' , x_train.shape[0])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded !\n",
            "Number of training images :  396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFNU-pHA_If6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.5, stratify=y_train)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYpTTfOpDwTQ",
        "outputId": "a26def2c-2ffb-4dff-c70b-7669272a0d45"
      },
      "source": [
        "no_of_classes = len(np.unique(y_train))\n",
        "no_of_classes"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFCqiXODrxb"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "y_train = np_utils.to_categorical(y_train,no_of_classes)\n",
        "y_test= np_utils.to_categorical(y_test,no_of_classes)\n",
        "# x_train = np_utils.to_categorical(x_train,no_of_classes)\n",
        "# x_test= np_utils.to_categorical(x_test,no_of_classes)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBbJyabLF1c8",
        "outputId": "fd10693e-d2af-45da-f494-5f148ca04142"
      },
      "source": [
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "\n",
        "def convert_image_to_array(files):\n",
        "    images_as_array=[]\n",
        "    for file in files:\n",
        "        # Convert to Numpy Array\n",
        "        images_as_array.append(img_to_array(load_img(file)))\n",
        "    return images_as_array\n",
        "\n",
        "x_train = np.array(convert_image_to_array(x_train))\n",
        "print('Shape of Training set: ',x_train.shape)\n",
        "\n",
        "x_test = np.array(convert_image_to_array(x_test))\n",
        "print('Shape of Test set : ',x_test.shape)\n",
        "\n",
        "print('Shape of one training image ',x_train[0].shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Training set:  (198, 100, 100, 3)\n",
            "Shape of Test set :  (198, 100, 100, 3)\n",
            "Shape of one training image  (100, 100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV4VzcaiEEDc"
      },
      "source": [
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GFR2y0i-Kak"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI7Pgt7wqbxz",
        "outputId": "eee4640e-767c-4795-bc8a-65ee9e113b59"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 12.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=9900bf586b398c879dd019a6885c9cecbfb286948b04542e57a01880da2467bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=aad129387cd045a91370f4d7b80e4519f4c68cf0496e44087b6e015f0e2c6d3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0VSV5526dhp"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDYYRtiE6iGx"
      },
      "source": [
        "def build_model(hp):  \n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Conv2D(\n",
        "        filters=hp.Int('conv_1_filter', min_value=16, max_value=128, step=16),\n",
        "        kernel_size=hp.Choice('conv_1_kernel', values = [2,5]),\n",
        "        activation='relu',\n",
        "        input_shape=(100,100,3)\n",
        "    ),\n",
        "    keras.layers.Conv2D(\n",
        "        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
        "        kernel_size=hp.Choice('conv_2_kernel', values = [2,5]),\n",
        "        activation='relu'\n",
        "    ),\n",
        "    # keras.layers.Conv2D(\n",
        "    #     filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
        "    #     kernel_size=hp.Choice('conv_2_kernel', values = [2,5]),\n",
        "    #     activation='relu'\n",
        "    # ),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(\n",
        "        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n",
        "        activation='relu'\n",
        "    ),\n",
        "    keras.layers.Dense(19, activation='softmax')\n",
        "  ])\n",
        "  \n",
        "  model.compile(optimizer=keras.optimizers.Adamax(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# def build_model(hp):  # random search passes this hyperparameter() object \n",
        "#     model = keras.models.Sequential()\n",
        "    \n",
        "#     model.add(Conv2D(hp.Int('input_units',\n",
        "#                                 min_value=16,\n",
        "#                                 max_value=64,\n",
        "#                                 step=16), (3, 3), input_shape=x_train.shape[1:]))\n",
        "    \n",
        "#     # model.add(Activation('relu'))\n",
        "#     # model.add(BatchNormalization())\n",
        "#     # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#     for i in range(hp.Int('n_layers', 1, 3)):  # adding variation of layers.\n",
        "#         model.add(Conv2D(hp.Int(f'conv_{i}_units',\n",
        "#                                 min_value=16,\n",
        "#                                 max_value=64,\n",
        "#                                 step=16), (3, 3)))\n",
        "#         model.add(Activation('relu'))\n",
        "\n",
        "#     model.add(Flatten()) \n",
        "#     # model.add(Dense(512,activation = 'relu'))\n",
        "#     # model.add(Dropout(0.5))\n",
        "#     model.add(Dense(128,activation = 'relu'))\n",
        "#     model.add(Dropout(0.5))\n",
        "#     model.add(Dense(19,activation = 'softmax'))\n",
        "\n",
        "\n",
        "#     model.compile(optimizer=keras.optimizers.Adamax(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
        "#                   loss=\"categorical_crossentropy\",\n",
        "#                   metrics=[\"accuracy\"])\n",
        "    \n",
        "#     return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAdyH02o6yI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177661a0-b224-407c-a4a7-2a218c3d54f7"
      },
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# tuner = RandomSearch(\n",
        "#     build_model,\n",
        "#     objective='val_accuracy',\n",
        "#     max_trials=10,\n",
        "#     directory='/content/drive/My Drive/Colab Notebooks',\n",
        "#     project_name='tuner')\n",
        "\n",
        "tuner = RandomSearch(build_model, objective='val_accuracy',\n",
        "                     max_trials=10,\n",
        "                     executions_per_trial=2,\n",
        "                     directory='/content/drive/My Drive/Colab Notebooks',\n",
        "                     project_name='tuner1')\n",
        "\n",
        "# print summary of the search space\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 6\n",
            "conv_1_filter (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 128, 'step': 16, 'sampling': None}\n",
            "conv_1_kernel (Choice)\n",
            "{'default': 2, 'conditions': [], 'values': [2, 5], 'ordered': True}\n",
            "conv_2_filter (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 64, 'step': 16, 'sampling': None}\n",
            "conv_2_kernel (Choice)\n",
            "{'default': 2, 'conditions': [], 'values': [2, 5], 'ordered': True}\n",
            "dense_1_units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 16, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001], 'ordered': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhQjaschV_bO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5aa395be-9f1d-4617-e8b5-f30921c98fb8"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD5YXM2i7K3y",
        "outputId": "10d926e0-ef0c-471b-8650-02fad2927e78"
      },
      "source": [
        "tuner.search(x_train, y_train, validation_data=(x_test, y_test), epochs=10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 02m 07s]\n",
            "val_accuracy: 0.8055555522441864\n",
            "\n",
            "Best val_accuracy So Far: 0.8510101139545441\n",
            "Total elapsed time: 00h 58m 59s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zxKdiCAEhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7abae9d7-3ebc-493d-eb01-63a65d8bf8c9"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in /content/drive/My Drive/Colab Notebooks/tuner1\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 128\n",
            "conv_1_kernel: 5\n",
            "conv_2_filter: 32\n",
            "conv_2_kernel: 2\n",
            "dense_1_units: 48\n",
            "learning_rate: 0.001\n",
            "Score: 0.8510101139545441\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 32\n",
            "conv_1_kernel: 2\n",
            "conv_2_filter: 48\n",
            "conv_2_kernel: 5\n",
            "dense_1_units: 112\n",
            "learning_rate: 0.001\n",
            "Score: 0.8484848439693451\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 32\n",
            "conv_1_kernel: 5\n",
            "conv_2_filter: 48\n",
            "conv_2_kernel: 2\n",
            "dense_1_units: 96\n",
            "learning_rate: 0.001\n",
            "Score: 0.8409090936183929\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 32\n",
            "conv_1_kernel: 5\n",
            "conv_2_filter: 64\n",
            "conv_2_kernel: 5\n",
            "dense_1_units: 112\n",
            "learning_rate: 0.001\n",
            "Score: 0.8383838534355164\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 16\n",
            "conv_1_kernel: 2\n",
            "conv_2_filter: 64\n",
            "conv_2_kernel: 2\n",
            "dense_1_units: 64\n",
            "learning_rate: 0.01\n",
            "Score: 0.8055555522441864\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 112\n",
            "conv_1_kernel: 5\n",
            "conv_2_filter: 32\n",
            "conv_2_kernel: 5\n",
            "dense_1_units: 48\n",
            "learning_rate: 0.001\n",
            "Score: 0.7045454680919647\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 64\n",
            "conv_1_kernel: 5\n",
            "conv_2_filter: 48\n",
            "conv_2_kernel: 2\n",
            "dense_1_units: 48\n",
            "learning_rate: 0.001\n",
            "Score: 0.6843434274196625\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 48\n",
            "conv_1_kernel: 2\n",
            "conv_2_filter: 48\n",
            "conv_2_kernel: 5\n",
            "dense_1_units: 112\n",
            "learning_rate: 0.01\n",
            "Score: 0.6843434274196625\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 48\n",
            "conv_1_kernel: 2\n",
            "conv_2_filter: 48\n",
            "conv_2_kernel: 5\n",
            "dense_1_units: 80\n",
            "learning_rate: 0.01\n",
            "Score: 0.6338383853435516\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 80\n",
            "conv_1_kernel: 5\n",
            "conv_2_filter: 64\n",
            "conv_2_kernel: 2\n",
            "dense_1_units: 96\n",
            "learning_rate: 0.01\n",
            "Score: 0.6111111044883728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m6KMHFrC1n2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36af2ec4-c69a-4b8a-87fa-b4eff502896e"
      },
      "source": [
        "models = tuner.get_best_models(num_models=2)\n",
        "models[0].summary()\n",
        "models[1].summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 96, 96, 128)       9728      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 95, 95, 32)        16416     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 288800)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 48)                13862448  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 19)                931       \n",
            "=================================================================\n",
            "Total params: 13,889,523\n",
            "Trainable params: 13,889,523\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 99, 99, 32)        416       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 95, 95, 48)        38448     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 433200)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 112)               48518512  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 19)                2147      \n",
            "=================================================================\n",
            "Total params: 48,559,523\n",
            "Trainable params: 48,559,523\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}